{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db1679d-a669-4500-a2f7-9776c8ab46a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "csv_file = 'C:/Users/DA/Desktop/Rokomari_Leaderboard_13.02.25.csv'\n",
    "links_df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "book_data = []\n",
    "\n",
    "# Iterate over each link in the CSV\n",
    "for link in links_df['Link']:  # Assuming the column name is 'Link'\n",
    "    # Send request to the URL\n",
    "    response = requests.get(link)\n",
    "\n",
    "    # If the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Check if the URL is the generic \"book\" page\n",
    "        if response.url == \"https://www.rokomari.com/book\":\n",
    "            # If it redirects to the generic page, mark it as \"Book Unavailable\"\n",
    "            book_data.append({'Link': link, 'Price': 'Book Unavailable'})\n",
    "        else:\n",
    "            # Parse the HTML content if it's a valid book page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the sell price on the page\n",
    "            price = soup.find('span', class_='sell-price')  # Assuming this is the correct class for price\n",
    "\n",
    "            if price:\n",
    "                # If price is found, save the link and the price\n",
    "                price = price.text.strip()\n",
    "                book_data.append({'Link': link, 'Price': price})\n",
    "            else:\n",
    "                # If price is not found, mark as \"Book Unavailable\"\n",
    "                book_data.append({'Link': link, 'Price': 'Book Unavailable'})\n",
    "\n",
    "# Create a new DataFrame from the extracted data\n",
    "output_df = pd.DataFrame(book_data)\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "output_df.to_csv('C:/Users/DA/Desktop/Rokomari_books_prices.csv', index=False)\n",
    "\n",
    "print(\"Data extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493c3a08-00fe-4166-befd-9ccf5eafb8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি পদ্মজা\n",
      "আমৃত্যু ভালোবাসি তোকে\n",
      "ভুল\n",
      "শামীম হুসাইন: ফ্রিল্যান্সার গড়ার কারিগর\n",
      "নিলী-নীলিমা\n",
      "তুলসীগঙ্গার তীরে....\n",
      "শঙ্খচূড় দ্বিতীয় খণ্ড\n",
      "ভালো নেই ভালোবাসা\n",
      "ভালোবাসারা ভালো নেই\n",
      "মেমসাহেব\n",
      "শৈলচূড়ায় চাঁদের হাসি\n",
      "কাংকাই ভলিউম-১\n",
      "দহনের দিনে জোছনার ফুল\n",
      "ডেকেছিল তাহারা যখন\n",
      "Fifteen Poems\n",
      "প্রেম প্রার্থনা মৃত্যু\n",
      "নিঃসঙ্গতার একশ বছর\n",
      "আসমান\n",
      "হন্ত্রক\n",
      "টুইটুবানির ফুল\n",
      "এমন দিনে তারে বলা যায়\n",
      "র রামজির বাহিনী\n",
      "দীর্ঘতম স্নানের কাছে\n",
      "ভালোবাসার উপবাস\n",
      "অন্তঃবাসিনী\n",
      "কোহিনূর\n",
      "ড্রাকুলা\n",
      "মোটিভেশনাল স্পিকারের হইলো মরিবার সাধ\n",
      "বেসিক আলী ১৭\n",
      "মোর ডেইজ অ্যাট দ্য মরিসাকি বুকশপ\n",
      "কুহেলিকা\n",
      "FLASH SALES\n",
      "এক হাজার টাকা\n",
      "লাভ ইন হুজুর\n",
      "আখেরি লড়াই\n",
      "পীরে কামেল\n",
      "দ্য প্রিন্সেস অফ উইঘুর\n",
      "কাবলার খালের রহস্যময় বৃক্ষ\n",
      "নোমান ৬.২\n",
      "নোমান ৬.১ আর ৬.২\n",
      "তোমার দেওয়া আমার কোন নাম ছিল না\n",
      "আবার কান্তার\n",
      "ব্ল্যাকমেইলার\n",
      "আই লাভ ইউ\n",
      "Secrets I've Held in my Heart\n",
      "প্রিয় সুখের আম্মু\n",
      "ইসরাইলের বন্দিনী\n",
      "উনকি\n",
      "কাঁটা ও ফুল\n",
      "তেজ: অরিজিন স্টোরি\n",
      "Art Of Triumph\n",
      "দ্যা গ্রোথ কোড\n",
      "অন্তিক মাহমুদ এর সব কমিক্স একসাথে\n",
      "সেক্স এডুকেশন\n",
      "গীটারনামা\n",
      "বাংলার আলোকচিত্রের বাস্তবতা অভিযান\n",
      "বিজনেস ব্লুপ্রিন্ট\n",
      "ইতিহাসের ছিন্নপত্র -তৃতীয় খণ্ড\n",
      "আমি মেজর ডালিম বলছি\n",
      "স্বাধীনতা-উত্তর বাংলাদেশ - প্রথম খণ্ড\n",
      "গ্রীষ্মের ছুটিতে দুঃস্বপ্ন জুলাই-আগস্ট ২০২৪\n",
      "ব্রেইন ব্যালেন্স ইকুয়াল ব্যাংক ব্যালেন্স\n",
      "দ্য মিস্ট্রি অফ বিজনেস ফেলিউর অ্যান্ড সাকসেস\n",
      "ছোটদের গ্রামার\n",
      "জামায়াতে ইসলামী\n",
      "দ্য নিটি গ্রিটি অব ফাইন্যান্স ম্যানেজমেন্ট ইন লাইফ\n",
      "ফাঁসির সেল থেকে দেখা বাংলাদেশ\n",
      "স্টোরিটেলিং ফর ব্র্যান্ডিং\n",
      "সময়ের সন্তানেরা : পঞ্জিকার পাতায় মানুষের ইতিহাস\n",
      "The Changing Nature of Geopolitics And The Futures Of Bangladesh\n",
      "স্বাধীনতা উত্তর বাংলাদেশ - ১ম খন্ড\n",
      "বাঙালির মিডিয়োক্রিটির সন্ধানে\n",
      "কাজী নজরুল ইসলামের ভাঙা গান\n",
      "বাংলাদেশের ছাপচিত্রকলা এবং তিনজন শিল্পী\n",
      "মাস্টারিং ইয়োর লাইফ\n",
      "ফ্রেশারস টু জব রেডি\n",
      "মার্কিন যুক্তরাষ্ট্র\n",
      "সফল শিক্ষকের গুণাবলি\n",
      "কথা বলি ইংরেজিতে মন খুলে\n",
      "যেতে যেতে তোমাকে কুড়াই\n",
      "ফিলিস্তিন\n",
      "ভ্রমণ রচনাবলি\n",
      "বাংলাদেশে 'র'\n",
      "বাংলাদেশ গণিত অলিম্পিয়াডের যত প্রশ্ন\n",
      "চব্বিশের গণঅভ্যুত্থান\n",
      "ইতিহাসের পথে পথে\n",
      "৩০ ডেজ ক্যারিয়ার থেরাপি\n",
      "প্রাপক আহমদ ছফা\n",
      "আন্দোলন সংগ্রামে জাসদ ও সমকালীন রাজনীতি\n",
      "সুন্দর হাতের লেখার সহজ অনুশীলন\n",
      "দ্য ডিসিপ্লিনড ট্রেডার\n",
      "বিজ্ঞানী ও বিজ্ঞান\n",
      "ট্রেডিং ইন দ্য জোন\n",
      "সাঈদ আহমাদ এর যত গজল\n",
      "Curing Banking Sector of Bangladesh\n",
      "এক্সোপ্ল্যানেট: বহিঃসৌরগ্রহের খোঁজে\n",
      "জীবনে যা দেখলাম - তৃতীয় খণ্ড\n",
      "লিংকড-ইনে ক্যারিয়ার\n",
      "৩৬ জুলাই ছাত্র - জনতার বিজয় ফ্যাসিবাদের পতন\n",
      "৩৬ জুলাই\n",
      "আল-কুরআনুল কারীম বাংলা অনুবাদ ও সংক্ষিপ্ত তাফসীর -\n",
      "আল-কুরআনের ভাষা শিক্ষা\n",
      "আল কোরআনের বৈজ্ঞানিক তাফসির: সূরা আল বাক্বারা\n",
      "আরবি ভাষা পাঠ - ১ম খণ্ড\n",
      "ইসলামি ইতিহাস\n",
      "আল বিদায়া ওয়ান নিহায়া\n",
      "ফিমেল মাইন্ড\n",
      "ডেইলি আমল সিরিজ\n",
      "আফগানিস্তানে আমি আল্লাহকে দেখেছি\n",
      "কুরআন তিলাওয়াতের নিয়ম নীতি\n",
      "কীভাবে আরবিতে কথা বলব\n",
      "সীরাহ প্রথম খণ্ড\n",
      "আমার নবি মুহাম্মাদ\n",
      "চিত্রসহ তাজবীদ শিক্ষা\n",
      "কষ্টিপাথর\n",
      "সিজদাহর বিজ্ঞান\n",
      "ভালোবাসা\n",
      "ফিকহুস সালাফ - ১ম খণ্ড\n",
      "উমরাহ সফরের গল্প\n",
      "প্রাণের কাবা\n",
      "রাগ নিয়ন্ত্রণ\n",
      "ইসলামী অর্থব্যবস্থার মূলনীতি\n",
      "সুফি দরবেশের হাকিকত\n",
      "মুসলিম জাতির ইতিহাস\n",
      "রুকইয়াহ\n",
      "মুমিন নারীর সারাদিন\n",
      "নবিজির ﷺ ঘর-সংসার\n",
      "শরীয়তের আলোকে পাত্র-পাত্রী নির্বাচন\n",
      "বছরব্যাপী করণীয় ও বর্জনীয়\n",
      "নারীদের তাফসিরুল কুরআন\n",
      "কুরআনিয়্যাত সিরিজ ১\n",
      "বাতেনিদের ইতিহাস\n",
      "হায়াতুস সাহাবা - ১ম খন্ড\n",
      "নারী সাহাবিদের জীবনকথা\n",
      "কুরআনীয় আরবী শিক্ষা\n",
      "দাড়ি মুমিনের সৌন্দর্য\n",
      "শামায়েলে তিরমিজি\n",
      "কালো পাথরের ছায়ায়\n",
      "জান্নাত লাভের আমল\n",
      "গাযযার সেই মেয়েটি\n",
      "সদাচরণ\n",
      "পুনর্জন্ম কেন সত্য!\n",
      "হাফেযে কুরআনের মর্যাদা\n",
      "আদাবুল মুতাআল্লিমীন\n",
      "নামাযে মুমিনের প্রাপ্তি\n",
      "ইসলামের অনুপম শিষ্টাচার\n",
      "বিশ্বব্যাপী ইহুদি চক্রান্ত\n",
      "রাহে সুন্নাত\n",
      "মহাকবি আল্লামা শেখ সাদী\n",
      "নামাজে কোথায় কী ভুল করি\n",
      "জাতীয় বিশ্ববিদ্যালয় ভর্তি সহায়িকা - মানবিক\n",
      "জাতীয় বিশ্ববিদ্যালয় ভর্তি সহায়িকা - ব্যবসায় শিক্ষা\n",
      "কিউএনএ প্লাস: ১০ দিনে চান্স নিশ্চিত - ভার্সিটি এ ইউনিট\n",
      "জাতীয় বিশ্ববিদ্যালয় ভর্তি সহায়িকা - বিজ্ঞান শাখা\n",
      "প্রশ্নব্যাংক জগন্নাথ বিশ্ববিদ্যালয়- B D ইউনিট\n",
      "প্রশ্নব্যাংক জাহাঙ্গীরনগর বিশ্ববিদ্যালয় D ইউনিট - বিজ্ঞান\n",
      "স্মার্ট ইংলিশ স্মার্ট ওয়ে টু লার্ন ইংলিশ\n",
      "আসপেক্ট সিরিজ ফার্মানলেজ - জাবি-ডি ইউনিট\n",
      "জাতীয় বিশ্ববিদ্যালয় ভর্তি পরীক্ষার প্রস্তুতিমূলক সহায়িকা ২০২৫ - মানবিক\n",
      "কিউএনএ সিইউ এ ইউনিট অ্যানালাইসিস মেগাবুক\n",
      "সহজ Biology\n",
      "জাহাঙ্গীরনগর’স GK\n",
      "জিকে আল্টিমেট - মডেল টেস্ট ও সাজেশন্স\n",
      "বিসিএস প্রিলিমিনারী অ্যানালাইসিস\n",
      "মিহির’স জিকে ফাইনাল সাজেশন ২০২৫ - সাধারণ জ্ঞান\n",
      "৪৭তম বিসিএস প্রিলি প্রিপারেশন ১৩টি বুক প্যাকেজ\n",
      "অভিযাত্রী বাংলা সাহিত্য ও ব্যাকরণ লিখিত ও এমসিকিউ\n",
      "বাংলা বিচিত্রা ১ম ও ২য় পত্র\n",
      "প্রশ্নব্যাংক রাজশাহী বিশ্ববিদ্যালয় C ইউনিট - বিজ্ঞান\n",
      "ছায়ামঞ্চ আই কিউ সামিট\n",
      "কাব্য ভাইয়ার ম্যাথ ট্রিকস\n",
      "কিউএনএ ভার্সিটি এ ইউনিট রিটেন মেগাবুক\n",
      "প্রশ্নব্যাংক রাজশাহী বিশ্ববিদ্যালয় A ইউনিট - মানবিক\n",
      "জাবি বুস্টার\n",
      "প্রাইমারি শিক্ষক নিয়োগ Analysis\n",
      "কিউএনএ সাস্ট এনালাইসিস মেগাবুক এ ইউনিট\n",
      "আসপেক্ট ইঞ্জিনিয়ারিং রিটেন - প্রশ্নব্যাংক-সাজেশন-মডেল টেস্ট\n",
      "কারেক্ট ইংলিশ প্রোনানসিয়েশন উইথ ফোনেটিক সিম্বল এনালাইসিস ইন বাংলা\n",
      "ইংলিশ বিচিত্রা\n",
      "EMINENT: BIBM :Bank Job Solution\n",
      "১৯ তম শিক্ষক নিবন্ধন Analysis\n",
      "শাহজালাল বিজ্ঞান ও প্রযুক্তি বিশ্ববিদ্যালয় প্রশ্নব্যাংক - B ইউনিট\n",
      "প্রিভিউ ভার্সিটি সলুশন\n",
      "বাংলা এ প্লাস - এইচএসসি ২০২৬\n",
      "জাতীয় বিশ্ববিদ্যালয় ভর্তি সহায়িকা ২০২৫ - মানবিক শাখা\n",
      "রাবি কনফার্ম প্রশ্নব্যাংক ও সাজেশন - মানবিক/বিভাগ পরিবর্তন\n",
      "শাহজালাল বিজ্ঞান ও প্রযুক্তি বিশ্ববিদ্যালয় প্রশ্নব্যাংক - A ইউনিট\n",
      "সাইফুর’স ৪৭তম বিসিএস প্রিলিমিনারি ডাইজেস্ট\n",
      "হালদা চট্টগ্রাম বিশ্ববিদ্যালয় ভর্তি প্রশ্নব্যাংক ও সাজেশন - এ ইউনিট\n",
      "পদার্থ বিচিত্রা ১ম ও ২য় পত্র\n",
      "পানকৌড়ি রাজশাহী বিশ্ববিদ্যালয় - ইউনিট-এ\n",
      "প্রশ্নব্যাংক চট্টগ্রাম বিশ্ববিদ্যালয় বিজ্ঞান - A ইউনিট\n",
      "গ্রামীণ ব্যাংক নিয়োগ সহায়িকা - MCQ,লিখিত\n",
      "ড্রাগ ইনডেক্স\n",
      "এসএসসি ২০২৫ পদার্থবিজ্ঞান প্রশ্নব্যাংক\n",
      "FLASH SALES\n",
      "আইন পাঠ\n",
      "সাধারণ জ্ঞান জয়কলি GK\n",
      "সংশপ্তক জাহাঙ্গীরনগর বিশ্ববিদ্যালয় প্রশ্নব্যাংক - সি ইউনিট\n",
      "পানকৌড়ি চট্টগ্রাম বিশ্ববিদ্যালয় - ইউনিট-বি/ডি\n",
      "\n",
      "Scraping complete! Data saved to C:\\Users\\DA\\Desktop\\Scraped_Books_title_bangla.csv\n"
     ]
    }
   ],
   "source": [
    "#Bangla title from link\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the CSV file containing book links\n",
    "csv_file = r\"C:\\Users\\DA\\Desktop\\TITLE TO LINK.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming the column containing URLs is named 'Link'\n",
    "book_titles = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Change 'Link' to the actual column name in your CSV\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the title from <h1>\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            title_text = h1_tag.get_text(strip=True, separator=\" \")\n",
    "            \n",
    "            # Remove text inside parentheses\n",
    "            cleaned_title = title_text.split(\" (\")[0]\n",
    "\n",
    "            book_titles.append({'URL': url, 'Title': cleaned_title})\n",
    "            print(f\"{cleaned_title}\")  # Debugging output\n",
    "        else:\n",
    "            book_titles.append({'URL': url, 'Title': 'Not Found'})\n",
    "            print(f\"Title not found for: {url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        book_titles.append({'URL': url, 'Title': 'Error'})\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output_file = r\"C:\\Users\\DA\\Desktop\\Scraped_Books_title_bangla.csv\"\n",
    "df_output = pd.DataFrame(book_titles)\n",
    "df_output.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nScraping complete! Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44fe3bda-4eb0-49b7-850c-005bd31c90d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromedriver found at: C:\\Users\\DA\\Desktop\\Jupiter notebook\\Driver\\chromedriver-win64\\chromedriver.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the correct path to your chromedriver executable\n",
    "chromedriver_path = r\"C:\\Users\\DA\\Desktop\\Jupiter notebook\\Driver\\chromedriver-win64\\chromedriver.exe\"  # Correct path\n",
    "\n",
    "# Verify if the path exists\n",
    "if not os.path.exists(chromedriver_path):\n",
    "    raise ValueError(f\"The specified chromedriver path does not exist: {chromedriver_path}\")\n",
    "\n",
    "# If exists, proceed with the rest of the script\n",
    "print(\"Chromedriver found at:\", chromedriver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4002dbf7-af3f-45db-87dd-b470431e2f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "csv_file = 'C:\\\\Users\\\\DA\\\\Desktop\\\\Rokomari\\\\22.02.25 roc scrap.csv'\n",
    "\n",
    "links_df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "book_data = []\n",
    "\n",
    "# Iterate over each link in the CSV\n",
    "for link in links_df['Link']:  # Assuming the column name is 'Link'\n",
    "    # Send request to the URL\n",
    "    response = requests.get(link)\n",
    "\n",
    "    # If the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Check if the URL is the generic \"book\" page\n",
    "        if response.url == \"https://www.rokomari.com/book\":\n",
    "            # If it redirects to the generic page, mark it as \"Book Unavailable\"\n",
    "            book_data.append({'Link': link, 'Price': 'Book Unavailable'})\n",
    "        else:\n",
    "            # Parse the HTML content if it's a valid book page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find the sell price on the page\n",
    "            price = soup.find('span', class_='sell-price')  # Assuming this is the correct class for price\n",
    "\n",
    "            if price:\n",
    "                # If price is found, save the link and the price\n",
    "                price = price.text.strip()\n",
    "                book_data.append({'Link': link, 'Price': price})\n",
    "            else:\n",
    "                # If price is not found, mark as \"Book Unavailable\"\n",
    "                book_data.append({'Link': link, 'Price': 'Book Unavailable'})\n",
    "\n",
    "# Create a new DataFrame from the extracted data\n",
    "output_df = pd.DataFrame(book_data)\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "output_df.to_csv('C:/Users/DA/Desktop/22.02.25_Rokomari_books_prices.csv', index=False)\n",
    "\n",
    "print(\"Data extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3498164d-878d-416a-8ab3-6479e4b9356f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
